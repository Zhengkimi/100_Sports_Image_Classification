{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import torch, gc\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.utils.data import Dataset\n",
    "from torchsummary import summary\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class SingleFolderDataset(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.folder_path = folder_path\n",
    "        self.image_list = sorted(os.listdir(folder_path))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.folder_path, self.image_list[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 100\n",
    "learning_rate = 2e-2\n",
    "num_epochs = 1000\n",
    "IMAGE_SIZE=256\n",
    "momentum=0.9\n",
    "weight_decay=1e-5\n",
    "BATCH_SIZE = 200\n",
    "RATIO=5\n",
    "SAVE_FREQ=10\n",
    "FROZEN_NUM=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "test_dataset = SingleFolderDataset(\"test\", transform=valid_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "test_num = list()\n",
    "for file_name in test_dataset.image_list:\n",
    "    test_num.append(int(file_name.split('.')[0]))\n",
    "test_indices = sorted(range(len(test_num)), key=lambda k: test_num[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /Users/Max/miniconda3/lib/python3.10/site-packages (8.0.150)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in /Users/Max/miniconda3/lib/python3.10/site-packages (from ultralytics) (3.6.3)\n",
      "Requirement already satisfied: numpy>=1.22.2 in /Users/Max/miniconda3/lib/python3.10/site-packages (from ultralytics) (1.25.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /Users/Max/miniconda3/lib/python3.10/site-packages (from ultralytics) (4.8.0.76)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /Users/Max/miniconda3/lib/python3.10/site-packages (from ultralytics) (9.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/Max/miniconda3/lib/python3.10/site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in /Users/Max/miniconda3/lib/python3.10/site-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/Max/miniconda3/lib/python3.10/site-packages (from ultralytics) (1.10.0)\n",
      "Requirement already satisfied: torch>=1.7.0 in /Users/Max/miniconda3/lib/python3.10/site-packages (from ultralytics) (2.0.1)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in /Users/Max/miniconda3/lib/python3.10/site-packages (from ultralytics) (0.15.2)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /Users/Max/miniconda3/lib/python3.10/site-packages (from ultralytics) (4.64.1)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /Users/Max/miniconda3/lib/python3.10/site-packages (from ultralytics) (1.5.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /Users/Max/miniconda3/lib/python3.10/site-packages (from ultralytics) (0.12.2)\n",
      "Requirement already satisfied: psutil in /Users/Max/miniconda3/lib/python3.10/site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in /Users/Max/miniconda3/lib/python3.10/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/Max/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/Max/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/Max/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/Max/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/Max/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (22.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/Max/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/Max/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.2.2->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/Max/miniconda3/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2022.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/Max/miniconda3/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/Max/miniconda3/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/Max/miniconda3/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Max/miniconda3/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2022.12.7)\n",
      "Requirement already satisfied: filelock in /Users/Max/miniconda3/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/Max/miniconda3/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (4.4.0)\n",
      "Requirement already satisfied: sympy in /Users/Max/miniconda3/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/Max/miniconda3/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/Max/miniconda3/lib/python3.10/site-packages (from torch>=1.7.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Max/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/Max/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.7.0->ultralytics) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/Max/miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.7.0->ultralytics) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-cls.pt to 'yolov8x-cls.pt'...\n",
      "100%|████████████████████████████████████████| 110M/110M [00:01<00:00, 76.4MB/s]\n",
      "New https://pypi.org/project/ultralytics/8.0.235 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.232 🚀 Python-3.8.10 torch-2.1.0+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24217MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=classify, mode=train, model=yolov8x-cls.pt, data=./, epochs=5000, time=None, patience=50, batch=256, imgsz=256, save=True, save_period=-1, cache=False, device=None, workers=16, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.2, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=2e-05, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=0.8, mixup=0.2, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/classify/train\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m /home/James24029775/DL/train... found 13992 images in 100 classes ✅ \n",
      "\u001b[34m\u001b[1mval:\u001b[0m /home/James24029775/DL/val... found 500 images in 100 classes ✅ \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m /home/James24029775/DL/test... found 500 images in 1 classes: ERROR ❌️ requires 100 classes, not 1\n",
      "Overriding model.yaml nc=1000 with nc=100\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n",
      "  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n",
      "  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n",
      "  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n",
      "  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n",
      "  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n",
      "  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n",
      "  7                  -1  1   7375360  ultralytics.nn.modules.conv.Conv             [640, 1280, 3, 2]             \n",
      "  8                  -1  3  27865600  ultralytics.nn.modules.block.C2f             [1280, 1280, 3, True]         \n",
      "  9                  -1  1   1769060  ultralytics.nn.modules.head.Classify         [1280, 100]                   \n",
      "YOLOv8x-cls summary: 183 layers, 56269940 parameters, 56269940 gradients, 154.4 GFLOPs\n",
      "Transferred 300/302 items from pretrained weights\n",
      "WARNING ⚠️ ClearML installed but not initialized correctly, not logging this run. It seems ClearML is not configured on this machine!\n",
      "To get started with ClearML, setup your own 'clearml-server' or create a free account at https://app.clear.ml\n",
      "Setup instructions can be found here: https://clear.ml/docs\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/classify/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n",
      "100%|██████████████████████████████████████| 6.23M/6.23M [00:00<00:00, 26.9MB/s]\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/James24029775/.virtualenvs/lab2/bin/yolo\", line 8, in <module>\n",
      "    sys.exit(entrypoint())\n",
      "  File \"/home/James24029775/.virtualenvs/lab2/lib/python3.8/site-packages/ultralytics/cfg/__init__.py\", line 448, in entrypoint\n",
      "    getattr(model, mode)(**overrides)  # default args from model\n",
      "  File \"/home/James24029775/.virtualenvs/lab2/lib/python3.8/site-packages/ultralytics/engine/model.py\", line 356, in train\n",
      "    self.trainer.train()\n",
      "  File \"/home/James24029775/.virtualenvs/lab2/lib/python3.8/site-packages/ultralytics/engine/trainer.py\", line 190, in train\n",
      "    self._do_train(world_size)\n",
      "  File \"/home/James24029775/.virtualenvs/lab2/lib/python3.8/site-packages/ultralytics/engine/trainer.py\", line 290, in _do_train\n",
      "    self._setup_train(world_size)\n",
      "  File \"/home/James24029775/.virtualenvs/lab2/lib/python3.8/site-packages/ultralytics/engine/trainer.py\", line 240, in _setup_train\n",
      "    self.amp = torch.tensor(check_amp(self.model), device=self.device)\n",
      "  File \"/home/James24029775/.virtualenvs/lab2/lib/python3.8/site-packages/ultralytics/utils/checks.py\", line 607, in check_amp\n",
      "    assert amp_allclose(YOLO('yolov8n.pt'), im)\n",
      "  File \"/home/James24029775/.virtualenvs/lab2/lib/python3.8/site-packages/ultralytics/utils/checks.py\", line 595, in amp_allclose\n",
      "    a = m(im, device=device, verbose=False)[0].boxes.data  # FP32 inference\n",
      "  File \"/home/James24029775/.virtualenvs/lab2/lib/python3.8/site-packages/ultralytics/engine/model.py\", line 98, in __call__\n",
      "    return self.predict(source, stream, **kwargs)\n",
      "  File \"/home/James24029775/.virtualenvs/lab2/lib/python3.8/site-packages/ultralytics/engine/model.py\", line 250, in predict\n",
      "    self.predictor.setup_model(model=self.model, verbose=is_cli)\n",
      "  File \"/home/James24029775/.virtualenvs/lab2/lib/python3.8/site-packages/ultralytics/engine/predictor.py\", line 320, in setup_model\n",
      "    self.model = AutoBackend(model or self.args.model,\n",
      "  File \"/home/James24029775/.virtualenvs/lab2/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/James24029775/.virtualenvs/lab2/lib/python3.8/site-packages/ultralytics/nn/autobackend.py\", line 123, in __init__\n",
      "    model = model.fuse(verbose=verbose) if fuse else model\n",
      "  File \"/home/James24029775/.virtualenvs/lab2/lib/python3.8/site-packages/ultralytics/nn/tasks.py\", line 135, in fuse\n",
      "    m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv\n",
      "  File \"/home/James24029775/.virtualenvs/lab2/lib/python3.8/site-packages/ultralytics/utils/torch_utils.py\", line 160, in fuse_conv_and_bn\n",
      "    fusedconv = nn.Conv2d(conv.in_channels,\n",
      "  File \"/home/James24029775/.virtualenvs/lab2/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1160, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/home/James24029775/.virtualenvs/lab2/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 832, in _apply\n",
      "    with torch.no_grad():\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!yolo task=classify mode=train model=yolov8x-cls.pt imgsz=256 data=./ batch=256 epochs=5000 lr0=0.00002 workers=16 pretrained=True cos_lr=True dropout=0.2 val=True hsv_h=0.015 hsv_s=0.7 hsv_v=0.4 mixup=0.2 mosaic=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n",
      "  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n",
      "  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n",
      "  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n",
      "  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n",
      "  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n",
      "  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n",
      "  7                  -1  1   7375360  ultralytics.nn.modules.conv.Conv             [640, 1280, 3, 2]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8                  -1  3  27865600  ultralytics.nn.modules.block.C2f             [1280, 1280, 3, True]         \n",
      "  9                  -1  1   2921960  ultralytics.nn.modules.head.Classify         [1280, 1000]                  \n",
      "YOLOv8x-cls summary: 183 layers, 57422840 parameters, 57422840 gradients, 155.3 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8x-cls.yaml\")  # build a new model from scratch\n",
    "model = YOLO(os.path.join(\"runs\", \"classify\", \"train7\", \"weights\", \"best.pt\") ) # load a pretrained model (recommended for training)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Probability Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 256x256 polo 1.00, horse racing 0.00, barell racing 0.00, harness racing 0.00, horse jumping 0.00, 5.5ms\n",
      "Speed: 0.1ms preprocess, 5.5ms inference, 20.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 sky surfing 0.99, snow boarding 0.00, bungee jumping 0.00, skydiving 0.00, high jump 0.00, 8.8ms\n",
      "Speed: 0.0ms preprocess, 8.8ms inference, 0.3ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 nascar racing 1.00, formula 1 racing 0.00, football 0.00, hockey 0.00, sidecar racing 0.00, 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 curling 0.99, speed skating 0.01, judo 0.00, snow boarding 0.00, luge 0.00, 5.4ms\n",
      "Speed: 0.0ms preprocess, 5.4ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 horse racing 0.86, barell racing 0.05, polo 0.05, horse jumping 0.02, harness racing 0.01, 6.2ms\n",
      "Speed: 0.0ms preprocess, 6.2ms inference, 0.3ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 baton twirling 1.00, javelin 0.00, figure skating women 0.00, golf 0.00, balance beam 0.00, 5.2ms\n",
      "Speed: 0.0ms preprocess, 5.2ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 cheerleading 0.93, figure skating pairs 0.06, rollerblade racing 0.00, basketball 0.00, skydiving 0.00, 5.1ms\n",
      "Speed: 0.0ms preprocess, 5.1ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 judo 1.00, fencing 0.00, pommel horse 0.00, curling 0.00, figure skating pairs 0.00, 4.5ms\n",
      "Speed: 0.0ms preprocess, 4.5ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 golf 0.99, field hockey 0.00, tennis 0.00, ampute football 0.00, croquet 0.00, 5.6ms\n",
      "Speed: 0.0ms preprocess, 5.6ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 baton twirling 0.47, golf 0.15, javelin 0.06, archery 0.03, pole vault 0.02, 5.7ms\n",
      "Speed: 0.0ms preprocess, 5.7ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 baseball 0.33, sumo wrestling 0.24, football 0.14, cheerleading 0.08, cricket 0.06, 5.2ms\n",
      "Speed: 0.0ms preprocess, 5.2ms inference, 0.3ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 wheelchair basketball 1.00, wheelchair racing 0.00, bike polo 0.00, track bicycle 0.00, pommel horse 0.00, 5.7ms\n",
      "Speed: 0.0ms preprocess, 5.7ms inference, 0.3ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 log rolling 1.00, water polo 0.00, water cycling 0.00, swimming 0.00, canoe slamon 0.00, 5.3ms\n",
      "Speed: 0.0ms preprocess, 5.3ms inference, 0.3ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 hydroplane racing 0.99, canoe slamon 0.01, sailboat racing 0.00, snowmobile racing 0.00, surfing 0.00, 4.8ms\n",
      "Speed: 0.0ms preprocess, 4.8ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 rings 1.00, trapeze 0.00, parallel bar 0.00, pommel horse 0.00, pole vault 0.00, 4.3ms\n",
      "Speed: 0.0ms preprocess, 4.3ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 wingsuit flying 1.00, hang gliding 0.00, figure skating men 0.00, luge 0.00, sky surfing 0.00, 5.9ms\n",
      "Speed: 0.0ms preprocess, 5.9ms inference, 0.3ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 snowmobile racing 1.00, motorcycle racing 0.00, bobsled 0.00, sidecar racing 0.00, hydroplane racing 0.00, 5.0ms\n",
      "Speed: 0.0ms preprocess, 5.0ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 balance beam 1.00, pommel horse 0.00, uneven bars 0.00, pole dancing 0.00, rings 0.00, 5.0ms\n",
      "Speed: 0.0ms preprocess, 5.0ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 uneven bars 1.00, parallel bar 0.00, balance beam 0.00, boxing 0.00, high jump 0.00, 5.1ms\n",
      "Speed: 0.0ms preprocess, 5.1ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 parallel bar 0.93, balance beam 0.03, pommel horse 0.02, uneven bars 0.00, hurdles 0.00, 5.3ms\n",
      "Speed: 0.0ms preprocess, 5.3ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 billiards 0.99, field hockey 0.00, croquet 0.00, gaga 0.00, table tennis 0.00, 4.9ms\n",
      "Speed: 0.0ms preprocess, 4.9ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 tug of war 1.00, cheerleading 0.00, gaga 0.00, chuckwagon racing 0.00, axe throwing 0.00, 4.9ms\n",
      "Speed: 0.0ms preprocess, 4.9ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 water cycling 1.00, tennis 0.00, bmx 0.00, fly fishing 0.00, sky surfing 0.00, 5.1ms\n",
      "Speed: 0.0ms preprocess, 5.1ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 water cycling 1.00, bmx 0.00, tennis 0.00, bike polo 0.00, horseshoe pitching 0.00, 5.2ms\n",
      "Speed: 0.0ms preprocess, 5.2ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 pole climbing 1.00, rock climbing 0.00, bungee jumping 0.00, axe throwing 0.00, archery 0.00, 7.2ms\n",
      "Speed: 0.0ms preprocess, 7.2ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 chuckwagon racing 1.00, harness racing 0.00, wheelchair racing 0.00, jousting 0.00, mushing 0.00, 8.2ms\n",
      "Speed: 0.0ms preprocess, 8.2ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 football 1.00, lacrosse 0.00, rugby 0.00, rollerblade racing 0.00, hockey 0.00, 4.6ms\n",
      "Speed: 0.0ms preprocess, 4.6ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 hurdles 1.00, gaga 0.00, parallel bar 0.00, horse jumping 0.00, volleyball 0.00, 4.9ms\n",
      "Speed: 0.0ms preprocess, 4.9ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 high jump 1.00, bungee jumping 0.00, olympic wrestling 0.00, pole vault 0.00, rings 0.00, 15.2ms\n",
      "Speed: 0.0ms preprocess, 15.2ms inference, 0.3ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 uneven bars 1.00, parallel bar 0.00, rings 0.00, pole vault 0.00, trapeze 0.00, 4.7ms\n",
      "Speed: 0.0ms preprocess, 4.7ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 disc golf 1.00, cricket 0.00, axe throwing 0.00, hammer throw 0.00, horse jumping 0.00, 5.6ms\n",
      "Speed: 0.0ms preprocess, 5.6ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 hydroplane racing 1.00, bobsled 0.00, luge 0.00, wingsuit flying 0.00, sailboat racing 0.00, 4.7ms\n",
      "Speed: 0.0ms preprocess, 4.7ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 pole dancing 1.00, pole climbing 0.00, jai alai 0.00, pole vault 0.00, baton twirling 0.00, 4.7ms\n",
      "Speed: 0.0ms preprocess, 4.7ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 volleyball 1.00, hammer throw 0.00, water polo 0.00, tennis 0.00, hurdles 0.00, 10.8ms\n",
      "Speed: 0.0ms preprocess, 10.8ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 lacrosse 1.00, jousting 0.00, ultimate 0.00, polo 0.00, baseball 0.00, 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 0.3ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 bike polo 1.00, wheelchair basketball 0.00, harness racing 0.00, wheelchair racing 0.00, track bicycle 0.00, 26.6ms\n",
      "Speed: 0.0ms preprocess, 26.6ms inference, 0.3ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 wheelchair basketball 1.00, wheelchair racing 0.00, basketball 0.00, bike polo 0.00, track bicycle 0.00, 4.4ms\n",
      "Speed: 0.0ms preprocess, 4.4ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 jousting 1.00, tug of war 0.00, lacrosse 0.00, polo 0.00, archery 0.00, 4.5ms\n",
      "Speed: 0.0ms preprocess, 4.5ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n",
      "\n",
      "0: 256x256 bungee jumping 1.00, skydiving 0.00, trapeze 0.00, tug of war 0.00, sky surfing 0.00, 4.6ms\n",
      "Speed: 0.0ms preprocess, 4.6ms inference, 0.2ms postprocess per image at shape (1, 3, 256, 256)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m tensorList \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images \u001b[38;5;129;01min\u001b[39;00m (test_loader):\n\u001b[1;32m      5\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(images)\n",
      "File \u001b[0;32m~/.virtualenvs/lab2/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.virtualenvs/lab2/lib/python3.8/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.virtualenvs/lab2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/lab2/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/lab2/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/.virtualenvs/lab2/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "label_pred = list()\n",
    "tensorList = []\n",
    "with torch.no_grad():\n",
    "    for images in (test_loader):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        index = outputs[0].probs.top5\n",
    "        prob = outputs[0].probs.top5conf\n",
    "        tensor = torch.zeros(1, 100)\n",
    "        for i in range(5):\n",
    "            tensor[0][index[i]] = prob[i]\n",
    "        tensorList.append(tensor)\n",
    "resultTensor = torch.cat(tensorList, dim=0)\n",
    "with open(os.path.join('yolov8-prob-vector.pickle'), 'wb') as file:\n",
    "    pickle.dump(resultTensor, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/yolov8-prob-vector.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m label_true \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest_label.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myolov8-prob-vector.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      3\u001b[0m     yoloV \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m      5\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myoloV8\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/yolov8-prob-vector.pickle'"
     ]
    }
   ],
   "source": [
    "label_true = pd.read_csv('Test_label.csv')\n",
    "with open(os.path.join(\"results\", \"yolov8-prob-vector.pickle\"), 'rb') as file:\n",
    "    yoloV = pickle.load(file)\n",
    "\n",
    "name = 'yoloV8'\n",
    "yoloV = yoloV.cpu().numpy()[test_indices,:]\n",
    "def acc_calculate(name,\n",
    "                  prob,\n",
    "                  label_true):\n",
    "    prob = prob_list[i]\n",
    "    prob = np.array(prob)\n",
    "    prob_label = pd.DataFrame(prob.argmax(axis=1), columns=['Category'])\n",
    "    acc = (sum(prob_label['Category'] == label_true['Category'])/500)\n",
    "    return_dict = {'model' : name, 'acc' : list(acc)}\n",
    "    return pd.DataFrame(return_dict).sort_values(by='acc')\n",
    "\n",
    "accuracy = acc_calculate(name, yoloV, )\n",
    "print(accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
